{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"86e1a5f3-b6b7-41a9-b524-8db298837058","_uuid":"88634006-e709-478c-8c17-403de7565be3","trusted":true},"source":["Task: Movie review classification\n","\n","Dataset: IMDB https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","Model: BERT\n","\n","Libraries: Pytorch, HuggingFace\n","\n","Reference: https://medium.com/@pyroswolf200/fine-tuning-bert-on-imdb-review-dataset-309e90b6dac0"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d4892de8-b155-4081-aae0-baed809279a2","_uuid":"34922613-1ce4-424a-b331-406367115576","trusted":true},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff350bb4-9bd1-43e2-b750-22e249b53b5e","_uuid":"2b8f5344-aaf9-45f2-8511-843262ea83dd","collapsed":false,"execution":{"iopub.execute_input":"2024-09-14T18:19:41.021433Z","iopub.status.busy":"2024-09-14T18:19:41.021042Z","iopub.status.idle":"2024-09-14T18:20:23.923495Z","shell.execute_reply":"2024-09-14T18:20:23.922406Z","shell.execute_reply.started":"2024-09-14T18:19:41.021396Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# %%capture\n","# !pip install wget\n","# !pip install transformers\n","# !pip install lightning"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"eb704e00-f503-4c6e-939c-a0c96852a707","_uuid":"b9492cc6-9df5-427e-bc53-81466f316a4a","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:01.531908Z","iopub.status.busy":"2024-09-08T19:00:01.531173Z","iopub.status.idle":"2024-09-08T19:00:06.195226Z","shell.execute_reply":"2024-09-08T19:00:06.193983Z","shell.execute_reply.started":"2024-09-08T19:00:01.531859Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","import torch\n"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:00:06.196824Z","iopub.status.busy":"2024-09-08T19:00:06.196380Z","iopub.status.idle":"2024-09-08T19:00:06.201453Z","shell.execute_reply":"2024-09-08T19:00:06.200465Z","shell.execute_reply.started":"2024-09-08T19:00:06.196786Z"},"trusted":true},"outputs":[],"source":["INPUT_CSV_PATH = \"IMDB Dataset.csv\" #\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","MAX_SEQ_LEN = 64\n","FAST_DEV_RUN = False\n","TOTAL_SAMPLES= 15000"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"cc800f38-062b-4838-8332-f9f02c8f1841","_uuid":"22f18a02-e698-4c91-9f07-f3d095262801","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:06.204171Z","iopub.status.busy":"2024-09-08T19:00:06.203836Z","iopub.status.idle":"2024-09-08T19:00:06.267909Z","shell.execute_reply":"2024-09-08T19:00:06.266944Z","shell.execute_reply.started":"2024-09-08T19:00:06.204138Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1 GPU(s) available. Using the GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"]}],"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'{torch.cuda.device_count()} GPU(s) available. Using the GPU: {torch.cuda.get_device_name(0)}')\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Using Mac ARM64 GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0b811ab5-c818-4607-aab7-62cebde52a34","_uuid":"5d38b7d1-59bf-4b66-ba01-50c2e82ba550","trusted":true},"source":["# Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"9a1c8382-7afe-4eda-b3a7-61c34177aeed","_uuid":"49716e03-c827-4271-bf37-43435bca94f9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:06.269197Z","iopub.status.busy":"2024-09-08T19:00:06.268919Z","iopub.status.idle":"2024-09-08T19:00:07.886387Z","shell.execute_reply":"2024-09-08T19:00:07.885444Z","shell.execute_reply.started":"2024-09-08T19:00:06.269166Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(INPUT_CSV_PATH)\n","df = df.head(TOTAL_SAMPLES)\n","df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"339d8e47-3d73-4010-a6dc-ee8b55e15451","_uuid":"bc34188d-123e-4213-9f35-48446c62aa31","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:07.888310Z","iopub.status.busy":"2024-09-08T19:00:07.887793Z","iopub.status.idle":"2024-09-08T19:00:11.411387Z","shell.execute_reply":"2024-09-08T19:00:11.410468Z","shell.execute_reply.started":"2024-09-08T19:00:07.888260Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8322</th>\n","      <td>this film is a good example of how through med...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8698</th>\n","      <td>summer holiday is the forgotten musical versio...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7264</th>\n","      <td>i bought this movie for 99 cents at k mart sev...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8241</th>\n","      <td>sequels well there are many reasons to make em...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>740</th>\n","      <td>what a waste of talent a very poor semi cohere...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 review  sentiment\n","8322  this film is a good example of how through med...          0\n","8698  summer holiday is the forgotten musical versio...          0\n","7264  i bought this movie for 99 cents at k mart sev...          0\n","8241  sequels well there are many reasons to make em...          0\n","740   what a waste of talent a very poor semi cohere...          0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]\n","def process(x):\n","    x = re.sub('[,\\.!?:()\"]', '', x)\n","    x = re.sub('<.*?>', ' ', x)\n","    x = re.sub('http\\S+', ' ', x)\n","    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n","    x = re.sub('\\s+', ' ', x)\n","    return x.lower().strip()\n","\n","df['review'] = df['review'].apply(lambda x: process(x))\n","\n","train, test = train_test_split(df, test_size=0.2)\n","\n","train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:00:11.412958Z","iopub.status.busy":"2024-09-08T19:00:11.412611Z","iopub.status.idle":"2024-09-08T19:00:11.418764Z","shell.execute_reply":"2024-09-08T19:00:11.417767Z","shell.execute_reply.started":"2024-09-08T19:00:11.412912Z"},"trusted":true},"outputs":[{"data":{"text/plain":["12000"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(train)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:00:11.420314Z","iopub.status.busy":"2024-09-08T19:00:11.419964Z","iopub.status.idle":"2024-09-08T19:00:11.430560Z","shell.execute_reply":"2024-09-08T19:00:11.429593Z","shell.execute_reply.started":"2024-09-08T19:00:11.420274Z"},"trusted":true},"outputs":[{"data":{"text/plain":["3000"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(test)"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"16882046-5d5c-453e-a248-a70c91a0aa10","_uuid":"5af2cd61-6b09-41ef-a0a2-ecc3f8970aad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.432144Z","iopub.status.busy":"2024-09-08T19:00:11.431795Z","iopub.status.idle":"2024-09-08T19:00:11.440682Z","shell.execute_reply":"2024-09-08T19:00:11.439815Z","shell.execute_reply.started":"2024-09-08T19:00:11.432104Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","train_sentences = train.review.values\n","train_labels = train.sentiment.values\n","test_sentences = test.review.values\n","test_labels = test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0fb16c8-1da1-47a7-bb55-3cbfbf074772","_uuid":"fc1d456c-f55f-4a74-bf9c-fcce8ca0ec0e","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.444241Z","iopub.status.busy":"2024-09-08T19:00:11.443964Z","iopub.status.idle":"2024-09-08T19:00:11.453275Z","shell.execute_reply":"2024-09-08T19:00:11.452266Z","shell.execute_reply.started":"2024-09-08T19:00:11.444211Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a70f72-0c55-4e82-acf1-141751e3ab49","_uuid":"ec3cdf96-cdcd-4bb3-8a40-a6f67832e332","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.454739Z","iopub.status.busy":"2024-09-08T19:00:11.454410Z","iopub.status.idle":"2024-09-08T19:00:11.463137Z","shell.execute_reply":"2024-09-08T19:00:11.462340Z","shell.execute_reply.started":"2024-09-08T19:00:11.454709Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_labels"]},{"cell_type":"markdown","metadata":{"_cell_guid":"984e06e2-7f62-4da5-8012-de9a3ab98c81","_uuid":"304daf0c-6119-4307-bee1-405950a54766","trusted":true},"source":["# Model"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"bc5fd136-7c2a-4a5a-b7c0-b22aaf996f98","_uuid":"829f9801-70ad-4bbe-ac04-61741232c5ad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.464764Z","iopub.status.busy":"2024-09-08T19:00:11.464204Z","iopub.status.idle":"2024-09-08T19:00:13.951942Z","shell.execute_reply":"2024-09-08T19:00:13.950998Z","shell.execute_reply.started":"2024-09-08T19:00:11.464722Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Loading BERT tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"f5d57b34-bc20-4bcb-8e56-6ff245739a25","_uuid":"3774dd7c-567b-47e8-8a75-03392b43fb11","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:13.953494Z","iopub.status.busy":"2024-09-08T19:00:13.953182Z","iopub.status.idle":"2024-09-08T19:00:13.961407Z","shell.execute_reply":"2024-09-08T19:00:13.959846Z","shell.execute_reply.started":"2024-09-08T19:00:13.953460Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","def generate_data(data,labels):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in data:\n","      # `encode_plus` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      #   (5) Pad or truncate the sentence to `max_length`\n","      #   (6) Create attention masks for [PAD] tokens.\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = MAX_SEQ_LEN,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      \n","      # Add the encoded sentence to the list.    \n","      input_ids.append(encoded_dict['input_ids'])\n","      \n","      # And its attention mask (simply differentiates padding from non-padding).\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Convert the lists into tensors.\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","\n","  return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"033dd0a0-92d5-425e-bcb5-e399f69b4ec9","_uuid":"c6b8d777-0575-42f8-a17b-753b65c2bd02","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:13.962989Z","iopub.status.busy":"2024-09-08T19:00:13.962613Z","iopub.status.idle":"2024-09-08T19:02:18.253793Z","shell.execute_reply":"2024-09-08T19:02:18.252871Z","shell.execute_reply.started":"2024-09-08T19:00:13.962939Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Original:  this film is a good example of how through media manipulation you can sell a film that is no more than a very unfunny tv sitcom in puerto rico the daily newspaper with the widest circulation has continuously written about the marvels of this film almost silencing all others coincidentally the newspaper with the second largest circulation belongs to the same owners the weekly claridad is the only newspaper on the island that has analyzed the film s form and content and pointed out all its flaws clich s and bad writing just because a film makes a portion of the audience laugh with easy and obvious jokes and because one can recognize actors and scenery does not make it an acceptable film\n","Token IDs: tensor([  101,  2023,  2143,  2003,  1037,  2204,  2742,  1997,  2129,  2083,\n","         2865, 16924,  2017,  2064,  5271,  1037,  2143,  2008,  2003,  2053,\n","         2062,  2084,  1037,  2200,  4895, 11263, 10695,  2100,  2694, 13130,\n","         1999,  5984,  7043,  1996,  3679,  3780,  2007,  1996,  2898,  3367,\n","         9141,  2038, 10843,  2517,  2055,  1996,  8348,  2015,  1997,  2023,\n","         2143,  2471,  9033,  7770,  6129,  2035,  2500, 27542,  1996,  3780,\n","         2007,  1996,  2117,   102])\n"]}],"source":["train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n","test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n","\n","print('Original: ', train_sentences[0])\n","print('Token IDs:', train_input_ids[0])"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"58d4180d-c063-42e8-a260-e80ddadfb8c6","_uuid":"38e7368e-d0cd-484b-8237-ec0bbc1585c5","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:02:18.255598Z","iopub.status.busy":"2024-09-08T19:02:18.255185Z","iopub.status.idle":"2024-09-08T19:02:18.262258Z","shell.execute_reply":"2024-09-08T19:02:18.261316Z","shell.execute_reply.started":"2024-09-08T19:02:18.255553Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = BATCH_SIZE, # Trains with this batch size.\n","            num_workers=3,\n","            persistent_workers=True,\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","test_dataloader = DataLoader(\n","            test_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = BATCH_SIZE, # Evaluate with this batch size.\n","            num_workers=3,\n","            persistent_workers=True,\n","        )"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:02:18.263705Z","iopub.status.busy":"2024-09-08T19:02:18.263376Z","iopub.status.idle":"2024-09-08T19:02:18.280380Z","shell.execute_reply":"2024-09-08T19:02:18.279553Z","shell.execute_reply.started":"2024-09-08T19:02:18.263675Z"},"trusted":true},"outputs":[],"source":["import logging \n","loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n","for logger in loggers:\n","    if \"transformers\" in logger.name.lower():\n","        logger.setLevel(logging.ERROR)\n","#https://stackoverflow.com/a/78844884"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["import lightning as L\n","from torch import nn\n","import torchmetrics\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch\n","import torch.optim as optim\n","from transformers import AdamW\n","from transformers import BertForSequenceClassification\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from lightning.pytorch.loggers import CSVLogger\n","import pandas as pd\n","from transformers import get_linear_schedule_with_warmup, PretrainedConfig\n","\n","\n","class Bert(L.LightningModule):\n","    def __init__(self, num_classes=None, training_steps=None, from_checkpoint=False, model_config_json_filepath=None):\n","        super().__init__()\n","        if from_checkpoint:\n","            model_config = PretrainedConfig.from_json_file(model_config_json_filepath)\n","            self._model = BertForSequenceClassification(config=model_config)\n","        else:\n","            self._model = BertForSequenceClassification.from_pretrained(\n","                \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n","                num_labels=num_classes,  # The number of output labels--2 for binary classification.\n","                # You can increase this for multi-class tasks.\n","                output_attentions=False,  # Whether the model returns attentions weights.\n","                output_hidden_states=False,  # Whether the model returns all hidden-states.\n","            )\n","        self.total_training_steps = training_steps\n","        self.f1 = torchmetrics.F1Score(task=\"binary\")\n","        self.confmat = torchmetrics.ConfusionMatrix(task=\"binary\", num_classes=2)\n","        \n","        #TODO remove\n","        self.preds = []\n","        self.labels = []\n","\n","    \n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, return_dict=False):\n","        outputs = self._model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, return_dict=return_dict)\n","        return outputs\n","    \n","    def training_step(self, batch, batch_idx):\n","        input_ids = batch[0]\n","        input_mask = batch[1]\n","        labels = batch[2]\n","        result = self(input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=input_mask, \n","                       labels=labels,\n","                       return_dict=True)\n","        loss = result.loss\n","        self.log(\"train_loss\", loss.item(), prog_bar=True)\n","\n","        logits = result.logits\n","\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        input_ids = batch[0]\n","        input_mask = batch[1]\n","        labels = batch[2]\n","        result = self(input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=input_mask, \n","                       labels=labels,\n","                       return_dict=True)\n","        loss = result.loss\n","        self.log(\"val_loss\", loss.item(), prog_bar=True)\n","        \n","        logits = result.logits\n","        y_hat = torch.argmax(logits, dim=1)\n","        self.f1.update(y_hat, labels)\n","        self.confmat.update(y_hat, labels)\n","        \n","        #TODO remove\n","        self.preds.extend(y_hat.cpu().numpy())\n","        self.labels.extend(labels.cpu().numpy())\n","\n","    def on_validation_epoch_end(self):\n","        f1_score = self.f1.compute()\n","        self.log('val_f1', f1_score, prog_bar=True)\n","        self.f1.reset()\n","\n","        confmat = self.confmat.compute()\n","        self.log(\"val_TN\", confmat[0,0], prog_bar=True)\n","        self.log(\"val_FP\", confmat[0,1], prog_bar=True)\n","        self.log(\"val_FN\", confmat[1,0], prog_bar=True)\n","        self.log(\"val_TP\", confmat[1,1], prog_bar=True)\n","        self.confmat.reset()\n","\n","        # Optionally log other metrics or average loss\n","        # avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        # self.log('val_loss', avg_loss, prog_bar=True)\n","        \n","        #TODO remove\n","        epoch = self.current_epoch\n","        df = pd.DataFrame({\n","            'actual_label': self.labels,\n","            'predicted_label': self.preds\n","        })\n","        df.to_csv(f'validation_predictions_epoch_{epoch}.csv', index=False)\n","        self.preds = []\n","        self.labels = []\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr = 2e-5, eps = 1e-8)\n","        lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                    num_warmup_steps = 0,\n","                                                    num_training_steps = self.total_training_steps)\n","        \n","        return {\"optimizer\":optimizer, \"lr_scheduler\":lr_scheduler}"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\n","  | Name    | Type                          | Params | Mode \n","------------------------------------------------------------------\n","0 | _model  | BertForSequenceClassification | 109 M  | eval \n","1 | f1      | BinaryF1Score                 | 0      | train\n","2 | confmat | BinaryConfusionMatrix         | 0      | train\n","------------------------------------------------------------------\n","109 M     Trainable params\n","0         Non-trainable params\n","109 M     Total params\n","437.935   Total estimated model params size (MB)\n","2         Modules in train mode\n","231       Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           "]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('val_TN', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'val_TN': ...})` instead.\n","c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('val_FP', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'val_FP': ...})` instead.\n","c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('val_FN', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'val_FN': ...})` instead.\n","c:\\Users\\Soumya Prabha Maiti\\Desktop\\ROOT\\Projects under development\\movie-review-classifier-bert\\env-cuda\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('val_TP', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'val_TP': ...})` instead.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 375/375 [02:25<00:00,  2.57it/s, v_num=0, train_loss=0.00134, val_loss=0.954, val_f1=0.825, val_TN=1151.0, val_FP=378.0, val_FN=173.0, val_TP=1298.0] "]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_epochs=10` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: 100%|██████████| 375/375 [02:30<00:00,  2.49it/s, v_num=0, train_loss=0.00134, val_loss=0.954, val_f1=0.825, val_TN=1151.0, val_FP=378.0, val_FN=173.0, val_TP=1298.0]\n"]}],"source":["lightning_model = Bert(num_classes=2,training_steps=len(train_dataloader)*EPOCHS)\n","logger = CSVLogger(\"logs\")\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=\"checkpoints\",\n","    save_top_k=-1,\n",")\n","trainer = L.Trainer(max_epochs=EPOCHS, fast_dev_run=FAST_DEV_RUN, accelerator=\"auto\", logger=logger, callbacks=[checkpoint_callback], gradient_clip_val=1.0)\n","trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["Bert(\n","  (_model): BertForSequenceClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSdpaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  )\n","  (f1): BinaryF1Score()\n","  (confmat): BinaryConfusionMatrix()\n",")"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["lightning_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:02:18.622874Z","iopub.status.idle":"2024-09-08T19:02:18.623204Z","shell.execute_reply":"2024-09-08T19:02:18.623054Z","shell.execute_reply.started":"2024-09-08T19:02:18.623036Z"},"trusted":true},"outputs":[],"source":["!ls logs/lightning_logs/version_0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:12:30.073715Z","iopub.status.busy":"2024-09-08T19:12:30.072985Z","iopub.status.idle":"2024-09-08T19:12:31.121365Z","shell.execute_reply":"2024-09-08T19:12:31.120209Z","shell.execute_reply.started":"2024-09-08T19:12:30.073674Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:13:58.588406Z","iopub.status.busy":"2024-09-08T19:13:58.587701Z","iopub.status.idle":"2024-09-08T19:14:13.721421Z","shell.execute_reply":"2024-09-08T19:14:13.720404Z","shell.execute_reply.started":"2024-09-08T19:13:58.588364Z"},"trusted":true},"outputs":[],"source":["# Steps to enable google drive file upload/download:\n","# 1. Create a project in google cloud console\n","# 2. Enable google drive api for the project at https://console.cloud.google.com/apis/library/drive.googleapis.com\n","# 3. Create a service account for the project at https://console.cloud.google.com/iam-admin/serviceaccounts\n","# 4. Download the json file containing the service account credentials\n","# 5. Share the google drive folder with the service account email with Editor permissions\n","# 6. pip install google-api-python-client==2.142.0\n","# 7. Set the environment variable GOOGLE_SERVICE_ACC_CREDS to the stringified json creds, or pass it as an argument to the functions\n","# 8. Run the functions\n","\n","import datetime\n","import io\n","import json\n","import os\n","\n","from dotenv import load_dotenv\n","from google.oauth2 import service_account\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","\n","class GDriveUtils:\n","    LOG_EVENTS = True\n","\n","    @staticmethod\n","    def get_gdrive_service(creds_stringified: str | None = None):\n","        SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n","        if not creds_stringified:\n","            print(\n","                \"Attempting to use google drive creds from environment variable\"\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            creds_stringified = os.getenv(\"GOOGLE_SERVICE_ACC_CREDS\")\n","        creds_dict = json.loads(creds_stringified)\n","        creds = service_account.Credentials.from_service_account_info(\n","            creds_dict, scopes=SCOPES\n","        )\n","        return build(\"drive\", \"v3\", credentials=creds)\n","\n","    @staticmethod\n","    def upload_file_to_gdrive(\n","        local_file_path,\n","        drive_parent_folder_id: str,\n","        drive_filename: str | None = None,\n","        creds_stringified: str | None = None,\n","    ) -> str:\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        if not drive_filename:\n","            drive_filename = os.path.basename(local_file_path)\n","\n","        file_metadata = {\n","            \"name\": drive_filename,\n","            \"parents\": [drive_parent_folder_id],\n","        }\n","        file = (\n","            service.files()\n","            .create(body=file_metadata, media_body=local_file_path)\n","            .execute()\n","        )\n","        print(\n","            \"File uploaded, drive file id: \", file.get(\"id\")\n","        ) if GDriveUtils.LOG_EVENTS else None\n","        return file.get(\"id\")\n","\n","    @staticmethod\n","    def upload_file_to_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        try:\n","            curr_time_utc = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","            file_name = f\"gdrive_upload_test_{curr_time_utc}_UTC.txt\"\n","            print(\n","                \"Creating local file to upload: \", file_name\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            with open(file_name, \"w\") as f:\n","                f.write(f\"gdrive_upload_test_{curr_time_utc}_UTC\")\n","            return GDriveUtils.upload_file_to_gdrive(\n","                file_name, drive_parent_folder_id, creds_stringified=creds_stringified\n","            )\n","        except Exception as e:\n","            raise e\n","        finally:\n","            if os.path.exists(file_name):\n","                print(\n","                    \"Deleting local file: \", file_name\n","                ) if GDriveUtils.LOG_EVENTS else None\n","                os.remove(file_name)\n","\n","    @staticmethod\n","    def download_file_from_gdrive(\n","        drive_file_id: str,\n","        local_file_path: str | None = None,\n","        creds_stringified: str | None = None,\n","    ):\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        drive_filename = service.files().get(fileId=drive_file_id, fields=\"name\").execute().get('name')\n","\n","        if not local_file_path:\n","            local_file_path = f\"{drive_file_id}_{drive_filename}\"\n","\n","        request = service.files().get_media(fileId=drive_file_id)\n","        file = io.BytesIO()\n","        downloader = MediaIoBaseDownload(file, request, chunksize= 25 * 1024 * 1024)\n","        done = False\n","        while done is False:\n","            status, done = downloader.next_chunk()\n","            print(f\"Downloading gdrive file {drive_filename} to local file {local_file_path}: {int(status.progress() * 100)}%.\") if GDriveUtils.LOG_EVENTS else None\n","\n","        if os.path.dirname(local_file_path):\n","            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n","        with open(local_file_path, \"wb\") as f:\n","            f.write(file.getvalue())\n","        print(\n","            \"Downloaded file locally to: \", local_file_path\n","        ) if GDriveUtils.LOG_EVENTS else None\n","\n","    @staticmethod\n","    def download_file_from_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        file_id = GDriveUtils.upload_file_to_gdrive_sanity_check(\n","            drive_parent_folder_id, creds_stringified\n","        )\n","        GDriveUtils.download_file_from_gdrive(\n","            file_id, creds_stringified=creds_stringified\n","        )\n","\n","    @staticmethod\n","    def stringify_json_creds(json_file: str, txt_file: str) -> str:\n","        with open(json_file, \"r\") as f:\n","            creds_dict = json.load(f)\n","        with open(txt_file, \"w\") as f:\n","            f.write(json.dumps(creds_dict))\n","\n","\n","# if __name__ == \"__main__\":\n","#     creds_stringified = input(\"Enter stringified creds: \")\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_0.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_1.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_2.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_3.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"logs/lightning_logs/version_0/metrics.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
