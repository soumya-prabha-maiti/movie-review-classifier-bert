{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"86e1a5f3-b6b7-41a9-b524-8db298837058","_uuid":"88634006-e709-478c-8c17-403de7565be3","trusted":true},"source":["Task: Movie review classification\n","\n","Dataset: IMDB https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","Model: BERT\n","\n","Libraries: Pytorch, HuggingFace\n","\n","Reference: https://medium.com/@pyroswolf200/fine-tuning-bert-on-imdb-review-dataset-309e90b6dac0\n","\n","Kaggle notebook: https://www.kaggle.com/code/soumyaprabhamaiti/finetune-bert-on-imdb-reviews-pytorch-lightning/edit"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d4892de8-b155-4081-aae0-baed809279a2","_uuid":"34922613-1ce4-424a-b331-406367115576","trusted":true},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff350bb4-9bd1-43e2-b750-22e249b53b5e","_uuid":"2b8f5344-aaf9-45f2-8511-843262ea83dd","collapsed":false,"execution":{"iopub.execute_input":"2024-09-14T18:19:41.021433Z","iopub.status.busy":"2024-09-14T18:19:41.021042Z","iopub.status.idle":"2024-09-14T18:20:23.923495Z","shell.execute_reply":"2024-09-14T18:20:23.922406Z","shell.execute_reply.started":"2024-09-14T18:19:41.021396Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install wget\n","!pip install transformers\n","!pip install lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb704e00-f503-4c6e-939c-a0c96852a707","_uuid":"b9492cc6-9df5-427e-bc53-81466f316a4a","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:01.531908Z","iopub.status.busy":"2024-09-08T19:00:01.531173Z","iopub.status.idle":"2024-09-08T19:00:06.195226Z","shell.execute_reply":"2024-09-08T19:00:06.193983Z","shell.execute_reply.started":"2024-09-08T19:00:01.531859Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:00:06.196824Z","iopub.status.busy":"2024-09-08T19:00:06.196380Z","iopub.status.idle":"2024-09-08T19:00:06.201453Z","shell.execute_reply":"2024-09-08T19:00:06.200465Z","shell.execute_reply.started":"2024-09-08T19:00:06.196786Z"},"trusted":true},"outputs":[],"source":["INPUT_CSV_PATH = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","MAX_SEQ_LEN = 64\n","FAST_DEV_RUN = False\n","TOTAL_SAMPLES= 50000"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc800f38-062b-4838-8332-f9f02c8f1841","_uuid":"22f18a02-e698-4c91-9f07-f3d095262801","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:06.204171Z","iopub.status.busy":"2024-09-08T19:00:06.203836Z","iopub.status.idle":"2024-09-08T19:00:06.267909Z","shell.execute_reply":"2024-09-08T19:00:06.266944Z","shell.execute_reply.started":"2024-09-08T19:00:06.204138Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'{torch.cuda.device_count()} GPU(s) available. Using the GPU: {torch.cuda.get_device_name(0)}')\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Using Mac ARM64 GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0b811ab5-c818-4607-aab7-62cebde52a34","_uuid":"5d38b7d1-59bf-4b66-ba01-50c2e82ba550","trusted":true},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a1c8382-7afe-4eda-b3a7-61c34177aeed","_uuid":"49716e03-c827-4271-bf37-43435bca94f9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:06.269197Z","iopub.status.busy":"2024-09-08T19:00:06.268919Z","iopub.status.idle":"2024-09-08T19:00:07.886387Z","shell.execute_reply":"2024-09-08T19:00:07.885444Z","shell.execute_reply.started":"2024-09-08T19:00:06.269166Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["df = pd.read_csv(INPUT_CSV_PATH)\n","df = df.head(TOTAL_SAMPLES)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"339d8e47-3d73-4010-a6dc-ee8b55e15451","_uuid":"bc34188d-123e-4213-9f35-48446c62aa31","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:07.888310Z","iopub.status.busy":"2024-09-08T19:00:07.887793Z","iopub.status.idle":"2024-09-08T19:00:11.411387Z","shell.execute_reply":"2024-09-08T19:00:11.410468Z","shell.execute_reply.started":"2024-09-08T19:00:07.888260Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]\n","def process(x):\n","    x = re.sub('[,\\.!?:()\"]', '', x)\n","    x = re.sub('<.*?>', ' ', x)\n","    x = re.sub('http\\S+', ' ', x)\n","    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n","    x = re.sub('\\s+', ' ', x)\n","    return x.lower().strip()\n","\n","df['review'] = df['review'].apply(lambda x: process(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[int(.8*len(df)), int(.9*len(df))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train, val, test = np.split(df.sample(frac=1, random_state=42), \n","                       [int(.8*len(df)), int(.9*len(df))])\n","print(len(train), len(val), len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16882046-5d5c-453e-a248-a70c91a0aa10","_uuid":"5af2cd61-6b09-41ef-a0a2-ecc3f8970aad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.432144Z","iopub.status.busy":"2024-09-08T19:00:11.431795Z","iopub.status.idle":"2024-09-08T19:00:11.440682Z","shell.execute_reply":"2024-09-08T19:00:11.439815Z","shell.execute_reply.started":"2024-09-08T19:00:11.432104Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","train_sentences = train.review.values\n","train_labels = train.sentiment.values\n","val_sentences = val.review.values\n","val_labels = val.sentiment.values\n","test_sentences = test.review.values\n","test_labels = test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0fb16c8-1da1-47a7-bb55-3cbfbf074772","_uuid":"fc1d456c-f55f-4a74-bf9c-fcce8ca0ec0e","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.444241Z","iopub.status.busy":"2024-09-08T19:00:11.443964Z","iopub.status.idle":"2024-09-08T19:00:11.453275Z","shell.execute_reply":"2024-09-08T19:00:11.452266Z","shell.execute_reply.started":"2024-09-08T19:00:11.444211Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a70f72-0c55-4e82-acf1-141751e3ab49","_uuid":"ec3cdf96-cdcd-4bb3-8a40-a6f67832e332","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.454739Z","iopub.status.busy":"2024-09-08T19:00:11.454410Z","iopub.status.idle":"2024-09-08T19:00:11.463137Z","shell.execute_reply":"2024-09-08T19:00:11.462340Z","shell.execute_reply.started":"2024-09-08T19:00:11.454709Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_labels"]},{"cell_type":"markdown","metadata":{"_cell_guid":"984e06e2-7f62-4da5-8012-de9a3ab98c81","_uuid":"304daf0c-6119-4307-bee1-405950a54766","trusted":true},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc5fd136-7c2a-4a5a-b7c0-b22aaf996f98","_uuid":"829f9801-70ad-4bbe-ac04-61741232c5ad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:11.464764Z","iopub.status.busy":"2024-09-08T19:00:11.464204Z","iopub.status.idle":"2024-09-08T19:00:13.951942Z","shell.execute_reply":"2024-09-08T19:00:13.950998Z","shell.execute_reply.started":"2024-09-08T19:00:11.464722Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5d57b34-bc20-4bcb-8e56-6ff245739a25","_uuid":"3774dd7c-567b-47e8-8a75-03392b43fb11","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:13.953494Z","iopub.status.busy":"2024-09-08T19:00:13.953182Z","iopub.status.idle":"2024-09-08T19:00:13.961407Z","shell.execute_reply":"2024-09-08T19:00:13.959846Z","shell.execute_reply.started":"2024-09-08T19:00:13.953460Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","def generate_data(data,labels):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in data:\n","      # `encode_plus` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      #   (5) Pad or truncate the sentence to `max_length`\n","      #   (6) Create attention masks for [PAD] tokens.\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = MAX_SEQ_LEN,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      \n","      # Add the encoded sentence to the list.    \n","      input_ids.append(encoded_dict['input_ids'])\n","      \n","      # And its attention mask (simply differentiates padding from non-padding).\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Convert the lists into tensors.\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","\n","  return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"033dd0a0-92d5-425e-bcb5-e399f69b4ec9","_uuid":"c6b8d777-0575-42f8-a17b-753b65c2bd02","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:00:13.962989Z","iopub.status.busy":"2024-09-08T19:00:13.962613Z","iopub.status.idle":"2024-09-08T19:02:18.253793Z","shell.execute_reply":"2024-09-08T19:02:18.252871Z","shell.execute_reply.started":"2024-09-08T19:00:13.962939Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n","val_input_ids, val_attention_masks,val_labels = generate_data(val_sentences,val_labels)\n","test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n","\n","print('Original: ', train_sentences[0])\n","print('Token IDs:', train_input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58d4180d-c063-42e8-a260-e80ddadfb8c6","_uuid":"38e7368e-d0cd-484b-8237-ec0bbc1585c5","collapsed":false,"execution":{"iopub.execute_input":"2024-09-08T19:02:18.255598Z","iopub.status.busy":"2024-09-08T19:02:18.255185Z","iopub.status.idle":"2024-09-08T19:02:18.262258Z","shell.execute_reply":"2024-09-08T19:02:18.261316Z","shell.execute_reply.started":"2024-09-08T19:02:18.255553Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = BATCH_SIZE, # Trains with this batch size.\n","            num_workers=3,\n","            persistent_workers=True,\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","val_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = BATCH_SIZE, # Evaluate with this batch size.\n","            num_workers=3,\n","            persistent_workers=True,\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","test_dataloader = DataLoader(\n","            test_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = BATCH_SIZE, # Evaluate with this batch size.\n","            num_workers=3,\n","            persistent_workers=True,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:02:18.263705Z","iopub.status.busy":"2024-09-08T19:02:18.263376Z","iopub.status.idle":"2024-09-08T19:02:18.280380Z","shell.execute_reply":"2024-09-08T19:02:18.279553Z","shell.execute_reply.started":"2024-09-08T19:02:18.263675Z"},"trusted":true},"outputs":[],"source":["import logging \n","loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n","for logger in loggers:\n","    if \"transformers\" in logger.name.lower():\n","        logger.setLevel(logging.ERROR)\n","#https://stackoverflow.com/a/78844884"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import lightning as L\n","from torch import nn\n","import torchmetrics\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch\n","import torch.optim as optim\n","from transformers import AdamW\n","from transformers import BertForSequenceClassification\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from lightning.pytorch.loggers import CSVLogger\n","import pandas as pd\n","from transformers import get_linear_schedule_with_warmup, PretrainedConfig\n","\n","\n","class Bert(L.LightningModule):\n","    def __init__(self, num_classes=None, training_steps=None, from_checkpoint=False, model_config_json_filepath=None):\n","        super().__init__()\n","        if from_checkpoint:\n","            model_config = PretrainedConfig.from_json_file(model_config_json_filepath)\n","            self._model = BertForSequenceClassification(config=model_config)\n","        else:\n","            self._model = BertForSequenceClassification.from_pretrained(\n","                \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n","                num_labels=num_classes,  # The number of output labels--2 for binary classification.\n","                # You can increase this for multi-class tasks.\n","                output_attentions=False,  # Whether the model returns attentions weights.\n","                output_hidden_states=False,  # Whether the model returns all hidden-states.\n","            )\n","        self.total_training_steps = training_steps\n","        self.f1 = torchmetrics.F1Score(task=\"binary\")\n","        self.confmat = torchmetrics.ConfusionMatrix(task=\"binary\", num_classes=2)\n","        \n","        #TODO remove\n","        self.preds = []\n","        self.labels = []\n","\n","    \n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, return_dict=False):\n","        outputs = self._model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, return_dict=return_dict)\n","        return outputs\n","    \n","    def _common_step(self, batch, batch_idx, prefix):\n","        input_ids = batch[0]\n","        input_mask = batch[1]\n","        labels = batch[2]\n","        result = self(input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=input_mask, \n","                       labels=labels,\n","                       return_dict=True)\n","        loss = result.loss\n","        self.log(f\"{prefix}_loss\", loss.item(), prog_bar=True)\n","        \n","        logits = result.logits\n","        y_hat = torch.argmax(logits, dim=1)\n","        self.f1.update(y_hat, labels)\n","        self.confmat.update(y_hat, labels)\n","        \n","        return loss, logits, y_hat, labels\n","    \n","    def training_step(self, batch, batch_idx):\n","        loss, logits, y_hat, y = self._common_step(batch, batch_idx, \"train\")\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        loss, logits, y_hat, y = self._common_step(batch, batch_idx, \"val\")\n","        \n","        #TODO remove\n","        self.preds.extend(y_hat.cpu().numpy())\n","        self.labels.extend(y.cpu().numpy())\n","    \n","    def test_step(self, batch, batch_idx):\n","        loss, logits, y_hat, y = self._common_step(batch, batch_idx, \"test\")\n","\n","    def _common_on_epoch_end(self, prefix):\n","        f1_score = self.f1.compute()\n","        self.log(f'{prefix}_f1', f1_score, prog_bar=True)\n","        self.f1.reset()\n","\n","        confmat = self.confmat.compute()\n","        self.log(f'{prefix}_TN', confmat[0,0], prog_bar=True)\n","        self.log(f'{prefix}_FP', confmat[0,1], prog_bar=True)\n","        self.log(f'{prefix}_FN', confmat[1,0], prog_bar=True)\n","        self.log(f'{prefix}_TP', confmat[1,1], prog_bar=True)\n","        self.confmat.reset()\n","\n","    def on_train_epoch_end(self):\n","        self._common_on_epoch_end(\"train\")\n","        \n","    def on_validation_epoch_end(self):\n","        self._common_on_epoch_end(\"val\")\n","\n","        # Optionally log other metrics or average loss\n","        # avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        # self.log('val_loss', avg_loss, prog_bar=True)\n","        \n","        #TODO remove\n","        epoch = self.current_epoch\n","        df = pd.DataFrame({\n","            'actual_label': self.labels,\n","            'predicted_label': self.preds\n","        })\n","        df.to_csv(f'validation_predictions_epoch_{epoch}.csv', index=False)\n","        self.preds = []\n","        self.labels = []\n","\n","    def on_test_epoch_end(self):\n","        self._common_on_epoch_end(\"test\")\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr = 2e-5, eps = 1e-8)\n","        lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                    num_warmup_steps = 0,\n","                                                    num_training_steps = self.total_training_steps)\n","        \n","        return {\"optimizer\":optimizer, \"lr_scheduler\":lr_scheduler}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lightning_model = Bert(num_classes=2,training_steps=len(train_dataloader)*EPOCHS)\n","logger = CSVLogger(\"logs\")\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=\"checkpoints\",\n","    save_top_k=-1,\n",")\n","trainer = L.Trainer(max_epochs=EPOCHS, fast_dev_run=FAST_DEV_RUN, accelerator=\"auto\", logger=logger, callbacks=[checkpoint_callback], gradient_clip_val=1.0)\n","trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.test(lightning_model, dataloaders=test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lightning_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:02:18.622874Z","iopub.status.idle":"2024-09-08T19:02:18.623204Z","shell.execute_reply":"2024-09-08T19:02:18.623054Z","shell.execute_reply.started":"2024-09-08T19:02:18.623036Z"},"trusted":true},"outputs":[],"source":["!ls logs/lightning_logs/version_0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:12:30.073715Z","iopub.status.busy":"2024-09-08T19:12:30.072985Z","iopub.status.idle":"2024-09-08T19:12:31.121365Z","shell.execute_reply":"2024-09-08T19:12:31.120209Z","shell.execute_reply.started":"2024-09-08T19:12:30.073674Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T19:13:58.588406Z","iopub.status.busy":"2024-09-08T19:13:58.587701Z","iopub.status.idle":"2024-09-08T19:14:13.721421Z","shell.execute_reply":"2024-09-08T19:14:13.720404Z","shell.execute_reply.started":"2024-09-08T19:13:58.588364Z"},"trusted":true},"outputs":[],"source":["# Steps to enable google drive file upload/download:\n","# 1. Create a project in google cloud console\n","# 2. Enable google drive api for the project at https://console.cloud.google.com/apis/library/drive.googleapis.com\n","# 3. Create a service account for the project at https://console.cloud.google.com/iam-admin/serviceaccounts\n","# 4. Download the json file containing the service account credentials\n","# 5. Share the google drive folder with the service account email with Editor permissions\n","# 6. pip install google-api-python-client==2.142.0\n","# 7. Set the environment variable GOOGLE_SERVICE_ACC_CREDS to the stringified json creds, or pass it as an argument to the functions\n","# 8. Run the functions\n","\n","import datetime\n","import io\n","import json\n","import os\n","\n","from dotenv import load_dotenv\n","from google.oauth2 import service_account\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","\n","class GDriveUtils:\n","    LOG_EVENTS = True\n","\n","    @staticmethod\n","    def get_gdrive_service(creds_stringified: str | None = None):\n","        SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n","        if not creds_stringified:\n","            print(\n","                \"Attempting to use google drive creds from environment variable\"\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            creds_stringified = os.getenv(\"GOOGLE_SERVICE_ACC_CREDS\")\n","        creds_dict = json.loads(creds_stringified)\n","        creds = service_account.Credentials.from_service_account_info(\n","            creds_dict, scopes=SCOPES\n","        )\n","        return build(\"drive\", \"v3\", credentials=creds)\n","\n","    @staticmethod\n","    def upload_file_to_gdrive(\n","        local_file_path,\n","        drive_parent_folder_id: str,\n","        drive_filename: str | None = None,\n","        creds_stringified: str | None = None,\n","    ) -> str:\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        if not drive_filename:\n","            drive_filename = os.path.basename(local_file_path)\n","\n","        file_metadata = {\n","            \"name\": drive_filename,\n","            \"parents\": [drive_parent_folder_id],\n","        }\n","        file = (\n","            service.files()\n","            .create(body=file_metadata, media_body=local_file_path)\n","            .execute()\n","        )\n","        print(\n","            \"File uploaded, drive file id: \", file.get(\"id\")\n","        ) if GDriveUtils.LOG_EVENTS else None\n","        return file.get(\"id\")\n","\n","    @staticmethod\n","    def upload_file_to_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        try:\n","            curr_time_utc = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","            file_name = f\"gdrive_upload_test_{curr_time_utc}_UTC.txt\"\n","            print(\n","                \"Creating local file to upload: \", file_name\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            with open(file_name, \"w\") as f:\n","                f.write(f\"gdrive_upload_test_{curr_time_utc}_UTC\")\n","            return GDriveUtils.upload_file_to_gdrive(\n","                file_name, drive_parent_folder_id, creds_stringified=creds_stringified\n","            )\n","        except Exception as e:\n","            raise e\n","        finally:\n","            if os.path.exists(file_name):\n","                print(\n","                    \"Deleting local file: \", file_name\n","                ) if GDriveUtils.LOG_EVENTS else None\n","                os.remove(file_name)\n","\n","    @staticmethod\n","    def download_file_from_gdrive(\n","        drive_file_id: str,\n","        local_file_path: str | None = None,\n","        creds_stringified: str | None = None,\n","    ):\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        drive_filename = service.files().get(fileId=drive_file_id, fields=\"name\").execute().get('name')\n","\n","        if not local_file_path:\n","            local_file_path = f\"{drive_file_id}_{drive_filename}\"\n","\n","        request = service.files().get_media(fileId=drive_file_id)\n","        file = io.BytesIO()\n","        downloader = MediaIoBaseDownload(file, request, chunksize= 25 * 1024 * 1024)\n","        done = False\n","        while done is False:\n","            status, done = downloader.next_chunk()\n","            print(f\"Downloading gdrive file {drive_filename} to local file {local_file_path}: {int(status.progress() * 100)}%.\") if GDriveUtils.LOG_EVENTS else None\n","\n","        if os.path.dirname(local_file_path):\n","            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n","        with open(local_file_path, \"wb\") as f:\n","            f.write(file.getvalue())\n","        print(\n","            \"Downloaded file locally to: \", local_file_path\n","        ) if GDriveUtils.LOG_EVENTS else None\n","\n","    @staticmethod\n","    def download_file_from_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        file_id = GDriveUtils.upload_file_to_gdrive_sanity_check(\n","            drive_parent_folder_id, creds_stringified\n","        )\n","        GDriveUtils.download_file_from_gdrive(\n","            file_id, creds_stringified=creds_stringified\n","        )\n","\n","    @staticmethod\n","    def stringify_json_creds(json_file: str, txt_file: str) -> str:\n","        with open(json_file, \"r\") as f:\n","            creds_dict = json.load(f)\n","        with open(txt_file, \"w\") as f:\n","            f.write(json.dumps(creds_dict))\n","\n","\n","# if __name__ == \"__main__\":\n","#     creds_stringified = input(\"Enter stringified creds: \")\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_0.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_1.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_2.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"validation_predictions_epoch_3.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","#     print(\n","#         GDriveUtils.upload_file_to_gdrive(\n","#             \"logs/lightning_logs/version_0/metrics.csv\", \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\", creds_stringified=creds_stringified\n","#         )\n","#     )\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
