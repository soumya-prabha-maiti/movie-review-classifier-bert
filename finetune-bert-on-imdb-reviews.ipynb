{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"86e1a5f3-b6b7-41a9-b524-8db298837058","_uuid":"88634006-e709-478c-8c17-403de7565be3","trusted":true},"source":["Task: Movie review classification\n","\n","Dataset: IMDB https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","Model: BERT\n","\n","Libraries: Pytorch, HuggingFace\n","\n","Reference: https://medium.com/@pyroswolf200/fine-tuning-bert-on-imdb-review-dataset-309e90b6dac0"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79027299-e48d-4442-b801-6cfa26c6f6e8","_uuid":"b3a3e5e4-8fb0-4831-baad-0b479184e3ab","trusted":true},"source":["# Config"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"776249ef-cae5-4a99-a061-e9422f206156","_uuid":"1dd48efa-eb12-4dbb-a3e5-9a30ebb7fa5f","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:04.158369Z","iopub.status.busy":"2024-09-04T17:10:04.158001Z","iopub.status.idle":"2024-09-04T17:10:04.169126Z","shell.execute_reply":"2024-09-04T17:10:04.168056Z","shell.execute_reply.started":"2024-09-04T17:10:04.158325Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["INPUT_CSV_PATH = \"IMDB Dataset.csv\"\n","EPOCHS = 1\n","BATCH_SIZE = 32\n","MAX_SEQ_LEN = 64"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d4892de8-b155-4081-aae0-baed809279a2","_uuid":"34922613-1ce4-424a-b331-406367115576","trusted":true},"source":["# Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"ff350bb4-9bd1-43e2-b750-22e249b53b5e","_uuid":"2b8f5344-aaf9-45f2-8511-843262ea83dd","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:04.170923Z","iopub.status.busy":"2024-09-04T17:10:04.170578Z","iopub.status.idle":"2024-09-04T17:10:34.308695Z","shell.execute_reply":"2024-09-04T17:10:34.307605Z","shell.execute_reply.started":"2024-09-04T17:10:04.170891Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install wget\n","!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"eb704e00-f503-4c6e-939c-a0c96852a707","_uuid":"b9492cc6-9df5-427e-bc53-81466f316a4a","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:34.311684Z","iopub.status.busy":"2024-09-04T17:10:34.310854Z","iopub.status.idle":"2024-09-04T17:10:38.473466Z","shell.execute_reply":"2024-09-04T17:10:38.472580Z","shell.execute_reply.started":"2024-09-04T17:10:34.311633Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"cc800f38-062b-4838-8332-f9f02c8f1841","_uuid":"22f18a02-e698-4c91-9f07-f3d095262801","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:38.474929Z","iopub.status.busy":"2024-09-04T17:10:38.474526Z","iopub.status.idle":"2024-09-04T17:10:38.537502Z","shell.execute_reply":"2024-09-04T17:10:38.536516Z","shell.execute_reply.started":"2024-09-04T17:10:38.474898Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1 GPU(s) available. Using the GPU: Tesla P100-PCIE-16GB\n"]}],"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'{torch.cuda.device_count()} GPU(s) available. Using the GPU: {torch.cuda.get_device_name(0)}')\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Using Mac ARM64 GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0b811ab5-c818-4607-aab7-62cebde52a34","_uuid":"5d38b7d1-59bf-4b66-ba01-50c2e82ba550","trusted":true},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"9a1c8382-7afe-4eda-b3a7-61c34177aeed","_uuid":"49716e03-c827-4271-bf37-43435bca94f9","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:38.540040Z","iopub.status.busy":"2024-09-04T17:10:38.539703Z","iopub.status.idle":"2024-09-04T17:10:40.061865Z","shell.execute_reply":"2024-09-04T17:10:40.060815Z","shell.execute_reply.started":"2024-09-04T17:10:38.540006Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(INPUT_CSV_PATH)\n","df = df.head(15000)\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"339d8e47-3d73-4010-a6dc-ee8b55e15451","_uuid":"bc34188d-123e-4213-9f35-48446c62aa31","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:40.063836Z","iopub.status.busy":"2024-09-04T17:10:40.063412Z","iopub.status.idle":"2024-09-04T17:10:43.729207Z","shell.execute_reply":"2024-09-04T17:10:43.728120Z","shell.execute_reply.started":"2024-09-04T17:10:40.063791Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6393</th>\n","      <td>the plot is about a female nurse named anna is...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12679</th>\n","      <td>over 21 the film version of the ruth gordon pl...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8821</th>\n","      <td>i saw this movie at the afi dallas festival mo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a wonderful little production the filming tech...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4966</th>\n","      <td>the lovely eva longoria parker plays kate who ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  review  sentiment\n","6393   the plot is about a female nurse named anna is...          0\n","12679  over 21 the film version of the ruth gordon pl...          1\n","8821   i saw this movie at the afi dallas festival mo...          0\n","1      a wonderful little production the filming tech...          1\n","4966   the lovely eva longoria parker plays kate who ...          0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]\n","def process(x):\n","    x = re.sub('[,\\.!?:()\"]', '', x)\n","    x = re.sub('<.*?>', ' ', x)\n","    x = re.sub('http\\S+', ' ', x)\n","    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n","    x = re.sub('\\s+', ' ', x)\n","    return x.lower().strip()\n","\n","df['review'] = df['review'].apply(lambda x: process(x))\n","\n","train, test = train_test_split(df, test_size=0.2)\n","\n","train.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"16882046-5d5c-453e-a248-a70c91a0aa10","_uuid":"5af2cd61-6b09-41ef-a0a2-ecc3f8970aad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:43.731086Z","iopub.status.busy":"2024-09-04T17:10:43.730670Z","iopub.status.idle":"2024-09-04T17:10:43.736534Z","shell.execute_reply":"2024-09-04T17:10:43.735658Z","shell.execute_reply.started":"2024-09-04T17:10:43.731039Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","train_sentences = train.review.values\n","train_labels = train.sentiment.values\n","test_sentences = test.review.values\n","test_labels = test.sentiment.values"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b0fb16c8-1da1-47a7-bb55-3cbfbf074772","_uuid":"fc1d456c-f55f-4a74-bf9c-fcce8ca0ec0e","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:43.738730Z","iopub.status.busy":"2024-09-04T17:10:43.738276Z","iopub.status.idle":"2024-09-04T17:10:43.759259Z","shell.execute_reply":"2024-09-04T17:10:43.758113Z","shell.execute_reply.started":"2024-09-04T17:10:43.738681Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/plain":["array(['the plot is about a female nurse named anna is caught in the middle of a world wide chaos as flesh eating zombies begin rising up and taking over the world and attacking the living she escapes into the streets and is rescued by a black police officer so far so good i usually enjoy horror movies but this piece of film doesn t deserve to be called horror it s not even thrilling just ridiculouseven the flintstones or kukla fran and ollie will give you more excitement it s like watching a bunch of bloodthirsty drunkards not being able to get into a shopping mall to by more liquor the heroes who has locked themselves in inside the shopping mall to avoid being eaten by the hoodlums outside are not better either even though they doesn t seem to be drunk they give the impression of being mentally disabled save your money instead of spending it on this',\n","       'over 21 the film version of the ruth gordon play which detailed her experiences trying to keep the marriage together with garson kanin after he d gone in the service provides irene dunne with one of her better later roles on the big screen it s also in keeping with what was then an upbeat spirit in america about how we would not screw up the peace as we did in the first world war and sow the seeds of yet another global conflict the play ruth gordon wrote and starred in herself ran for 221 performances in 1944 on broadway and was confined simply to the bungalow that gordon and harvey stephens who was the male lead had on a training base if you look on the broadway credits list it says that the production was staged by george s kaufman as opposed to being directed by him i m not sure of the distinction but i can imagine that with a wit and will as strong as kaufman s it must have been an interesting period putting the production together before opening night when columbia bought the screen rights sidney buchman had to do some considerable script reconstruction to move the action beyond the bungalow the film bears very little trace of its stage origins alexander knox plays the husband and charles coburn the employer of both dunne and knox who are writers knox has graduated to not only editor but featured columnist his words and thoughts help sell the paper and coburn is in a bind but knox feels he has to get into the war the seminal event of his time in order to speak authoritatively on the kind of post war world he wants this was not an uncommon theme in those years irene dunne has some good comic moments the kind she used to have when she was appearing opposite cary grant in fact garson kanin directed both of them in my favorite wife a few years earlier coburn is his usual cantankerous old water buffalo of a boss who ultimately has a good heart over 21 was an optimistic picture which sad to say wasn t accurate about what the allies and i mean all of them could bring to the peace conferences to create a better world still hopefully a new generation will get it right',\n","       'i saw this movie at the afi dallas festival most of the audience including my wife enjoyed this comedy drama but i didn t it stars lucas haas brick alpha dog molly parker kissed the five senses hollywoodland and adam scott first snow art school confidential the director is matt bissonnette who s married to molly parker all three actors do a fine job in this movie about 3 friends the marriage of two of them and infidelity involving the third it all takes place at a lake house and it looks wonderful the film wants to treat its subject as a comedy first and then a drama and i thought it needed to be the other way around',\n","       ...,\n","       'in a summer that also boasted such repugnant stinkers as snakes on a plane and the da vinci code that s a pretty bold statement but i stand by it nonetheless superman returns like king kong 6 months before it is overlong hyper indulgent and with cgi up to the eyeballs my god this stuff is doing my head in richard donner had the idea of keep it real for his 2 outings and i do find his approach to the special and optical effects to be the most appropriate brian singer bombards us with so much cgi that it really takes you out of the story and constantly reminds you that you are watching a wannabe blockbuster that thinks that the only way to impress an audience is to spend 250 million a totally irresponsible amount of money on obnoxious visual effects that don t live up to the hype we ve seen everything and been everywhere that cgi can take us there s no real atmosphere or involvement in this and for a film that is 95 made up of this crapwell you figure it out i ve read so many reviews from fanboy critics about how the movie has soul or a human heart or tender character moments puh lease we ve already had brooding superheros silently screaming you d love me if you knew who i am dozens of times already in recent years and sr offers absolutely nothing new in this regard even the plot is recycled garbage lex luthor a seriously mis cast and hammy kevin spacey plotting to destroy the landmass of america was done in the first film already and wellthat s your lot it s amazing that they managed to draw out this junk to 25 painful hours even if the cast were likable it would make it less unbearable but brandon routh has the on screen personality of a mahogany hat stand kate bosworth is completely unconvincing as a pulitzer prize winning journalist james marsden is 250 wooden as usual and kevin spacey really needs to either fire his agent or acquire some better judgement the only cast member i liked was the lovely parker posey but i m into weird looking girls every year films like this get bigger and more bombastic pretty soon we ll have 300 million films studios need to realise that maybe they should start looking down instead of looking up for all the money that warner spent on this pile of crap for all the resources that this movie cost to makewas it worth it in my opinion certainly not this garbage has put me of superman for life',\n","       'goof factual error when charlie walks out of the room to commit suicide he takes his gun with a silencer after a few seconds we hear a loud bang from the same gun being fired',\n","       'i love cartoons they can show things that films with real actors and scenery cannot though computer effects are changing that more and more they can push the boundaries of satire the simpsons good taste south park spectacle aladdin or reality toy story there are some good examples of this in ice age such as when we see a motley herd of now extinct mammals migrating across countryside chatting like old friends such scenes are a pleasure to watch as we get the feeling of both the familiar and the strange at the same time usually in a way that makes us laugh while ice age is not as good as the top animated movies of all time it s a really fun film sit back enjoy the deliberate anachronisms the lovely backgrounds and the belly laughs the story follows manfred the grumpy mammoth sid an idiotic sloth and diego a sabretooth tiger as they take a human baby back to his tribe for very different reasons on the way naturally they have a whole lot of problems also popping up throughout the journey is scrat history s unluckiest rodent who is desperately trying to bury an acorn for the winter in glaciers on top of dead trees in ice caves his opening scene is a classic it s a simple story with a very predictable end and a middle that is just a series of funny incidents with some character building moments thrown in but some of the scenes such as the nappy changing or the dodos are hilarious the animals are likeable and it looks good there is one quite touching moment too when cave paintings of mammoths come to life in front of manfred s eyes not a must see but good for a fun hour or so 7 10'],\n","      dtype=object)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_sentences"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"54a70f72-0c55-4e82-acf1-141751e3ab49","_uuid":"ec3cdf96-cdcd-4bb3-8a40-a6f67832e332","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:43.761415Z","iopub.status.busy":"2024-09-04T17:10:43.760765Z","iopub.status.idle":"2024-09-04T17:10:43.769580Z","shell.execute_reply":"2024-09-04T17:10:43.768557Z","shell.execute_reply.started":"2024-09-04T17:10:43.761371Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1, 0, ..., 0, 1, 1])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_labels"]},{"cell_type":"markdown","metadata":{"_cell_guid":"984e06e2-7f62-4da5-8012-de9a3ab98c81","_uuid":"304daf0c-6119-4307-bee1-405950a54766","trusted":true},"source":["# Model"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"bc5fd136-7c2a-4a5a-b7c0-b22aaf996f98","_uuid":"829f9801-70ad-4bbe-ac04-61741232c5ad","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:43.771131Z","iopub.status.busy":"2024-09-04T17:10:43.770841Z","iopub.status.idle":"2024-09-04T17:10:46.355493Z","shell.execute_reply":"2024-09-04T17:10:46.354448Z","shell.execute_reply.started":"2024-09-04T17:10:43.771099Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading BERT tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3e27889722e40cc8ccb7e5e746a3ab9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"526300e0a8ba4a59b265804151250e73","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5f7ec8deb4b408bb147866b7854f857","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bada304972884413a1a6b585676131ec","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"f5d57b34-bc20-4bcb-8e56-6ff245739a25","_uuid":"3774dd7c-567b-47e8-8a75-03392b43fb11","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:46.360076Z","iopub.status.busy":"2024-09-04T17:10:46.359747Z","iopub.status.idle":"2024-09-04T17:10:46.367554Z","shell.execute_reply":"2024-09-04T17:10:46.366516Z","shell.execute_reply.started":"2024-09-04T17:10:46.360037Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","def generate_data(data,labels):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in data:\n","      # `encode_plus` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      #   (5) Pad or truncate the sentence to `max_length`\n","      #   (6) Create attention masks for [PAD] tokens.\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = MAX_SEQ_LEN,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      \n","      # Add the encoded sentence to the list.    \n","      input_ids.append(encoded_dict['input_ids'])\n","      \n","      # And its attention mask (simply differentiates padding from non-padding).\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Convert the lists into tensors.\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","\n","  return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"033dd0a0-92d5-425e-bcb5-e399f69b4ec9","_uuid":"c6b8d777-0575-42f8-a17b-753b65c2bd02","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:10:46.369475Z","iopub.status.busy":"2024-09-04T17:10:46.369140Z","iopub.status.idle":"2024-09-04T17:12:51.179901Z","shell.execute_reply":"2024-09-04T17:12:51.178915Z","shell.execute_reply.started":"2024-09-04T17:10:46.369442Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Original:  the plot is about a female nurse named anna is caught in the middle of a world wide chaos as flesh eating zombies begin rising up and taking over the world and attacking the living she escapes into the streets and is rescued by a black police officer so far so good i usually enjoy horror movies but this piece of film doesn t deserve to be called horror it s not even thrilling just ridiculouseven the flintstones or kukla fran and ollie will give you more excitement it s like watching a bunch of bloodthirsty drunkards not being able to get into a shopping mall to by more liquor the heroes who has locked themselves in inside the shopping mall to avoid being eaten by the hoodlums outside are not better either even though they doesn t seem to be drunk they give the impression of being mentally disabled save your money instead of spending it on this\n","Token IDs: tensor([  101,  1996,  5436,  2003,  2055,  1037,  2931,  6821,  2315,  4698,\n","         2003,  3236,  1999,  1996,  2690,  1997,  1037,  2088,  2898,  8488,\n","         2004,  5771,  5983, 14106,  4088,  4803,  2039,  1998,  2635,  2058,\n","         1996,  2088,  1998,  7866,  1996,  2542,  2016, 12976,  2046,  1996,\n","         4534,  1998,  2003, 10148,  2011,  1037,  2304,  2610,  2961,  2061,\n","         2521,  2061,  2204,  1045,  2788,  5959,  5469,  5691,  2021,  2023,\n","         3538,  1997,  2143,   102])\n"]}],"source":["train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n","test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n","\n","print('Original: ', train_sentences[0])\n","print('Token IDs:', train_input_ids[0])"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"58d4180d-c063-42e8-a260-e80ddadfb8c6","_uuid":"38e7368e-d0cd-484b-8237-ec0bbc1585c5","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:51.181491Z","iopub.status.busy":"2024-09-04T17:12:51.181195Z","iopub.status.idle":"2024-09-04T17:12:51.188506Z","shell.execute_reply":"2024-09-04T17:12:51.187646Z","shell.execute_reply.started":"2024-09-04T17:12:51.181459Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = BATCH_SIZE # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","test_dataloader = DataLoader(\n","            test_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = BATCH_SIZE # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"d126a499-d253-46d6-bb5b-9e6a9c70a2eb","_uuid":"8213c77f-e72e-495e-9285-d26ea2f371fa","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:51.190093Z","iopub.status.busy":"2024-09-04T17:12:51.189813Z","iopub.status.idle":"2024-09-04T17:12:54.275545Z","shell.execute_reply":"2024-09-04T17:12:54.274643Z","shell.execute_reply.started":"2024-09-04T17:12:51.190061Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"223c165890ab4c01860a9af6bd31650a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.to(device)"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"5734e9d5-1a54-41ab-9baa-72069c59d7a8","_uuid":"71f3a2ac-c1c1-4b9c-9880-6c1c9e6060d2","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.277144Z","iopub.status.busy":"2024-09-04T17:12:54.276696Z","iopub.status.idle":"2024-09-04T17:12:54.731899Z","shell.execute_reply":"2024-09-04T17:12:54.731080Z","shell.execute_reply.started":"2024-09-04T17:12:54.277111Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"ae32b164-378a-431c-a07a-88ce6cfb32f7","_uuid":"2cbf9965-0ba6-478f-bb3b-c51bf36a28bf","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.733469Z","iopub.status.busy":"2024-09-04T17:12:54.733013Z","iopub.status.idle":"2024-09-04T17:12:54.738458Z","shell.execute_reply":"2024-09-04T17:12:54.737449Z","shell.execute_reply.started":"2024-09-04T17:12:54.733435Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * EPOCHS\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":17,"metadata":{"_cell_guid":"71f900e2-61e0-498c-9a76-ab6db0656fb7","_uuid":"388e7801-7f04-47fc-82df-563afb49b76c","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.739907Z","iopub.status.busy":"2024-09-04T17:12:54.739489Z","iopub.status.idle":"2024-09-04T17:12:54.751367Z","shell.execute_reply":"2024-09-04T17:12:54.750481Z","shell.execute_reply.started":"2024-09-04T17:12:54.739873Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"1978ef6e-9ef8-49ac-945d-2e5219c632b1","_uuid":"aa9a6933-b746-4804-9fa2-8e0828fc547c","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.753112Z","iopub.status.busy":"2024-09-04T17:12:54.752549Z","iopub.status.idle":"2024-09-04T17:12:54.760789Z","shell.execute_reply":"2024-09-04T17:12:54.759899Z","shell.execute_reply.started":"2024-09-04T17:12:54.753070Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"a9c23aac-59bf-4b3a-912c-3e60a8de0a29","_uuid":"2452da05-7e4a-4cd5-be28-8093fed63e9b","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.762115Z","iopub.status.busy":"2024-09-04T17:12:54.761812Z","iopub.status.idle":"2024-09-04T17:12:54.772524Z","shell.execute_reply":"2024-09-04T17:12:54.771671Z","shell.execute_reply.started":"2024-09-04T17:12:54.762077Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model.device"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"cbc4b560-ed9c-4ac8-a25f-1a31a4f60dce","_uuid":"3f167a8a-1f19-4118-ba50-53e618438824","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.773856Z","iopub.status.busy":"2024-09-04T17:12:54.773518Z","iopub.status.idle":"2024-09-04T17:12:54.783868Z","shell.execute_reply":"2024-09-04T17:12:54.783020Z","shell.execute_reply.started":"2024-09-04T17:12:54.773825Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/plain":["{'base_lrs': [2e-05],\n"," 'last_epoch': 0,\n"," 'verbose': False,\n"," '_step_count': 1,\n"," '_get_lr_called_within_step': False,\n"," '_last_lr': [2e-05],\n"," 'lr_lambdas': [{}]}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["scheduler.state_dict()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"88e2db9f-4a30-4c44-abfb-9bce400aeb26","_uuid":"ef08a321-661e-497f-9593-3c64157f066b","trusted":true},"source":["## Training"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"0e362956-4d87-401c-bae0-0ef4352be57f","_uuid":"f12a2469-3be9-4901-a78b-6f3bed5eabb7","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:12:54.785886Z","iopub.status.busy":"2024-09-04T17:12:54.785237Z","iopub.status.idle":"2024-09-04T17:14:05.260364Z","shell.execute_reply":"2024-09-04T17:14:05.259394Z","shell.execute_reply.started":"2024-09-04T17:12:54.785844Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 1 ========\n","Training...\n","  Batch    40  of    375.    Elapsed: 0:00:08.\n","  Batch    80  of    375.    Elapsed: 0:00:16.\n","  Batch   120  of    375.    Elapsed: 0:00:23.\n","  Batch   160  of    375.    Elapsed: 0:00:30.\n","  Batch   200  of    375.    Elapsed: 0:00:38.\n","  Batch   240  of    375.    Elapsed: 0:00:45.\n","  Batch   280  of    375.    Elapsed: 0:00:53.\n","  Batch   320  of    375.    Elapsed: 0:01:00.\n","  Batch   360  of    375.    Elapsed: 0:01:08.\n","\n","  Average training loss: 0.45\n","  Training epcoh took: 0:01:10\n","\n","Training complete!\n","Total training took 0:01:10 (h:mm:ss)\n"]}],"source":["import random\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, EPOCHS):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    \n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","       \n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","\n","        model.zero_grad()        \n","\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","\n","    t0 = time.time()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,   \n","            'Training Time': training_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"64dbe041-da64-418e-8ad9-15998a62c979","_uuid":"49b68e7f-b40e-49e2-a0b7-63c92737c4c6","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:14:05.262481Z","iopub.status.busy":"2024-09-04T17:14:05.261819Z","iopub.status.idle":"2024-09-04T17:14:07.189218Z","shell.execute_reply":"2024-09-04T17:14:07.188409Z","shell.execute_reply.started":"2024-09-04T17:14:05.262435Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(),f\"BERT-imdb-epoch-{EPOCHS}.pt\")\n","torch.save(optimizer.state_dict(),f\"BERT-optimizer-imdb-epoch-{EPOCHS}.pt\")\n","torch.save(scheduler.state_dict(),f\"BERT-scheduler-imdb-epoch-{EPOCHS}.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["temp"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T17:33:38.431034Z","iopub.status.busy":"2024-09-04T17:33:38.430569Z","iopub.status.idle":"2024-09-04T17:33:41.095785Z","shell.execute_reply":"2024-09-04T17:33:41.094676Z","shell.execute_reply.started":"2024-09-04T17:33:38.430997Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["pytorch-ignite                           0.5.1\n","pytorch-lightning                        2.4.0\n","torch                                    2.4.0\n","torchaudio                               2.4.0\n","torchinfo                                1.8.0\n","torchmetrics                             1.4.1\n","torchvision                              0.19.0\n"]}],"source":["!pip list|grep torch"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T17:33:46.969338Z","iopub.status.busy":"2024-09-04T17:33:46.968551Z","iopub.status.idle":"2024-09-04T17:33:49.631846Z","shell.execute_reply":"2024-09-04T17:33:49.630529Z","shell.execute_reply.started":"2024-09-04T17:33:46.969296Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["transformers                             4.44.0\n"]}],"source":["!pip list|grep transformers"]},{"cell_type":"code","execution_count":30,"metadata":{"_cell_guid":"529e10dd-3b9e-4566-9452-6c6149a67f21","_uuid":"13c39541-cfbd-4f23-a82e-7d555707b2be","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:17:32.656228Z","iopub.status.busy":"2024-09-04T17:17:32.655369Z","iopub.status.idle":"2024-09-04T17:17:32.669848Z","shell.execute_reply":"2024-09-04T17:17:32.667002Z","shell.execute_reply.started":"2024-09-04T17:17:32.656189Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/plain":["False"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["\"bert.embeddings.position_ids\" in model.state_dict().keys()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T17:19:40.286188Z","iopub.status.busy":"2024-09-04T17:19:40.284843Z","iopub.status.idle":"2024-09-04T17:19:40.915440Z","shell.execute_reply":"2024-09-04T17:19:40.914438Z","shell.execute_reply.started":"2024-09-04T17:19:40.286129Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n","A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model2 = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels=2,\n","    output_attentions=False,\n","    output_hidden_states=False,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T17:20:45.265583Z","iopub.status.busy":"2024-09-04T17:20:45.265182Z","iopub.status.idle":"2024-09-04T17:20:45.432144Z","shell.execute_reply":"2024-09-04T17:20:45.431123Z","shell.execute_reply.started":"2024-09-04T17:20:45.265546Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model2.to(device)\n","model2.device\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T17:22:39.857162Z","iopub.status.busy":"2024-09-04T17:22:39.856158Z","iopub.status.idle":"2024-09-04T17:22:40.179470Z","shell.execute_reply":"2024-09-04T17:22:40.178537Z","shell.execute_reply.started":"2024-09-04T17:22:39.857120Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/3987815190.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model2.load_state_dict(torch.load(\"/kaggle/working/BERT-imdb-epoch-1.pt\", map_location=device))\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["model2.load_state_dict(torch.load(\"/kaggle/working/BERT-imdb-epoch-1.pt\", map_location=device))\n","model2.eval()"]},{"cell_type":"markdown","metadata":{},"source":["temp"]},{"cell_type":"code","execution_count":24,"metadata":{"_cell_guid":"9440534e-4301-4108-97a9-e3fbcf631705","_uuid":"f95fd293-9bdf-43a4-a907-933d236ed196","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:14:09.272937Z","iopub.status.busy":"2024-09-04T17:14:09.272282Z","iopub.status.idle":"2024-09-04T17:14:14.206359Z","shell.execute_reply":"2024-09-04T17:14:14.205417Z","shell.execute_reply.started":"2024-09-04T17:14:09.272895Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Preds are ready\n"]}],"source":["# Prediction on test set\n","\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in test_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","prediction_set = []\n","\n","for i in range(len(true_labels)):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  prediction_set.append(pred_labels_i)\n","\n","prediction_scores = [item for sublist in prediction_set for item in sublist]\n","\n","print('Preds are ready')"]},{"cell_type":"code","execution_count":25,"metadata":{"_cell_guid":"8adae04d-cfe1-4be7-9a95-5b47bfa8f8d4","_uuid":"a4847853-4059-4b94-97ef-603c60c08d75","collapsed":false,"execution":{"iopub.execute_input":"2024-09-04T17:14:14.208575Z","iopub.status.busy":"2024-09-04T17:14:14.207937Z","iopub.status.idle":"2024-09-04T17:14:14.231192Z","shell.execute_reply":"2024-09-04T17:14:14.230379Z","shell.execute_reply.started":"2024-09-04T17:14:14.208525Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[1275  247]\n"," [ 260 1218]]\n","precision_score: 0.8313993174061434\n","recall_score: 0.8240866035182679\n","f1_score: 0.8309389689677974\n"]}],"source":["from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n","\n","cm = confusion_matrix(test_labels, prediction_scores)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","print(\"precision_score:\",precision_score(test_labels, prediction_scores))\n","print(\"recall_score:\",recall_score(test_labels, prediction_scores))\n","print(\"f1_score:\",f1_score(test_labels, prediction_scores, average='macro'))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
