{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"86e1a5f3-b6b7-41a9-b524-8db298837058","_uuid":"88634006-e709-478c-8c17-403de7565be3","trusted":true},"source":["Task: Movie review classification\n","\n","Dataset: IMDB https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","Model: BERT\n","\n","Libraries: Pytorch, HuggingFace\n","\n","Reference: https://medium.com/@pyroswolf200/fine-tuning-bert-on-imdb-review-dataset-309e90b6dac0"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d4892de8-b155-4081-aae0-baed809279a2","_uuid":"34922613-1ce4-424a-b331-406367115576","trusted":true},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff350bb4-9bd1-43e2-b750-22e249b53b5e","_uuid":"2b8f5344-aaf9-45f2-8511-843262ea83dd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install wget\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb704e00-f503-4c6e-939c-a0c96852a707","_uuid":"b9492cc6-9df5-427e-bc53-81466f316a4a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["INPUT_CSV_PATH = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","MAX_SEQ_LEN = 512\n","TOTAL_SAMPLES = 15000\n","DRIVE_FOLDER_ID = \"16QGpGwyIbA29BJa8uLMD1lMCrnOb1_Gj\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","GOOGLE_SERVICE_ACC_TOKEN = user_secrets.get_secret(\"GOOGLE_SERVICE_ACC_TOKEN\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'{torch.cuda.device_count()} GPU(s) available. Using the GPU: {torch.cuda.get_device_name(0)}')\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","    print(\"Using Mac ARM64 GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using CPU')"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Steps to enable google drive file upload/download:\n","# 1. Create a project in google cloud console\n","# 2. Enable google drive api for the project at https://console.cloud.google.com/apis/library/drive.googleapis.com\n","# 3. Create a service account for the project at https://console.cloud.google.com/iam-admin/serviceaccounts\n","# 4. Download the json file containing the service account credentials\n","# 5. Share the google drive folder with the service account email with Editor permissions\n","# 6. pip install google-api-python-client==2.142.0\n","# 7. Set the environment variable GOOGLE_SERVICE_ACC_CREDS to the stringified json creds, or pass it as an argument to the functions\n","# 8. Run the functions\n","\n","import datetime\n","import io\n","import json\n","import os\n","\n","from dotenv import load_dotenv\n","from google.oauth2 import service_account\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","\n","class GDriveUtils:\n","    LOG_EVENTS = True\n","\n","    @staticmethod\n","    def get_gdrive_service(creds_stringified: str | None = None):\n","        SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n","        if not creds_stringified:\n","            print(\n","                \"Attempting to use google drive creds from environment variable\"\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            creds_stringified = os.getenv(\"GOOGLE_SERVICE_ACC_CREDS\")\n","        creds_dict = json.loads(creds_stringified)\n","        creds = service_account.Credentials.from_service_account_info(\n","            creds_dict, scopes=SCOPES\n","        )\n","        return build(\"drive\", \"v3\", credentials=creds)\n","\n","    @staticmethod\n","    def upload_file_to_gdrive(\n","        local_file_path,\n","        drive_parent_folder_id: str,\n","        drive_filename: str | None = None,\n","        creds_stringified: str | None = None,\n","    ) -> str:\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        if not drive_filename:\n","            drive_filename = os.path.basename(local_file_path)\n","\n","        file_metadata = {\n","            \"name\": drive_filename,\n","            \"parents\": [drive_parent_folder_id],\n","        }\n","        file = (\n","            service.files()\n","            .create(body=file_metadata, media_body=local_file_path)\n","            .execute()\n","        )\n","        print(\n","            \"File uploaded, drive file id: \", file.get(\"id\")\n","        ) if GDriveUtils.LOG_EVENTS else None\n","        return file.get(\"id\")\n","\n","    @staticmethod\n","    def upload_file_to_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        try:\n","            curr_time_utc = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","            file_name = f\"gdrive_upload_test_{curr_time_utc}_UTC.txt\"\n","            print(\n","                \"Creating local file to upload: \", file_name\n","            ) if GDriveUtils.LOG_EVENTS else None\n","            with open(file_name, \"w\") as f:\n","                f.write(f\"gdrive_upload_test_{curr_time_utc}_UTC\")\n","            return GDriveUtils.upload_file_to_gdrive(\n","                file_name, drive_parent_folder_id, creds_stringified=creds_stringified\n","            )\n","        except Exception as e:\n","            raise e\n","        finally:\n","            if os.path.exists(file_name):\n","                print(\n","                    \"Deleting local file: \", file_name\n","                ) if GDriveUtils.LOG_EVENTS else None\n","                os.remove(file_name)\n","\n","    @staticmethod\n","    def download_file_from_gdrive(\n","        drive_file_id: str,\n","        local_file_path: str | None = None,\n","        creds_stringified: str | None = None,\n","    ):\n","        service = GDriveUtils.get_gdrive_service(creds_stringified)\n","\n","        drive_filename = service.files().get(fileId=drive_file_id, fields=\"name\").execute().get('name')\n","\n","        if not local_file_path:\n","            local_file_path = f\"{drive_file_id}_{drive_filename}\"\n","\n","        request = service.files().get_media(fileId=drive_file_id)\n","        file = io.BytesIO()\n","        downloader = MediaIoBaseDownload(file, request, chunksize= 25 * 1024 * 1024)\n","        done = False\n","        while done is False:\n","            status, done = downloader.next_chunk()\n","            print(f\"Downloading gdrive file {drive_filename} to local file {local_file_path}: {int(status.progress() * 100)}%.\") if GDriveUtils.LOG_EVENTS else None\n","\n","        if os.path.dirname(local_file_path):\n","            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n","        with open(local_file_path, \"wb\") as f:\n","            f.write(file.getvalue())\n","        print(\n","            \"Downloaded file locally to: \", local_file_path\n","        ) if GDriveUtils.LOG_EVENTS else None\n","\n","    @staticmethod\n","    def download_file_from_gdrive_sanity_check(\n","        drive_parent_folder_id: str,\n","        creds_stringified: str | None = None,\n","    ):\n","        file_id = GDriveUtils.upload_file_to_gdrive_sanity_check(\n","            drive_parent_folder_id, creds_stringified\n","        )\n","        GDriveUtils.download_file_from_gdrive(\n","            file_id, creds_stringified=creds_stringified\n","        )\n","\n","    @staticmethod\n","    def stringify_json_creds(json_file: str, txt_file: str) -> str:\n","        with open(json_file, \"r\") as f:\n","            creds_dict = json.load(f)\n","        with open(txt_file, \"w\") as f:\n","            f.write(json.dumps(creds_dict))\n","\n","\n","print(\n","    GDriveUtils.upload_file_to_gdrive_sanity_check(\n","        DRIVE_FOLDER_ID, GOOGLE_SERVICE_ACC_TOKEN\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0b811ab5-c818-4607-aab7-62cebde52a34","_uuid":"5d38b7d1-59bf-4b66-ba01-50c2e82ba550","trusted":true},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a1c8382-7afe-4eda-b3a7-61c34177aeed","_uuid":"49716e03-c827-4271-bf37-43435bca94f9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["df = pd.read_csv(INPUT_CSV_PATH)\n","df = df.head(TOTAL_SAMPLES)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"339d8e47-3d73-4010-a6dc-ee8b55e15451","_uuid":"bc34188d-123e-4213-9f35-48446c62aa31","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]\n","def process(x):\n","    x = re.sub('[,\\.!?:()\"]', '', x)\n","    x = re.sub('<.*?>', ' ', x)\n","    x = re.sub('http\\S+', ' ', x)\n","    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n","    x = re.sub('\\s+', ' ', x)\n","    return x.lower().strip()\n","\n","df['review'] = df['review'].apply(lambda x: process(x))\n","\n","train, test = train_test_split(df, test_size=0.2)\n","\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16882046-5d5c-453e-a248-a70c91a0aa10","_uuid":"5af2cd61-6b09-41ef-a0a2-ecc3f8970aad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","train_sentences = train.review.values\n","train_labels = train.sentiment.values\n","test_sentences = test.review.values\n","test_labels = test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0fb16c8-1da1-47a7-bb55-3cbfbf074772","_uuid":"fc1d456c-f55f-4a74-bf9c-fcce8ca0ec0e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a70f72-0c55-4e82-acf1-141751e3ab49","_uuid":"ec3cdf96-cdcd-4bb3-8a40-a6f67832e332","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_labels"]},{"cell_type":"markdown","metadata":{"_cell_guid":"984e06e2-7f62-4da5-8012-de9a3ab98c81","_uuid":"304daf0c-6119-4307-bee1-405950a54766","trusted":true},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc5fd136-7c2a-4a5a-b7c0-b22aaf996f98","_uuid":"829f9801-70ad-4bbe-ac04-61741232c5ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5d57b34-bc20-4bcb-8e56-6ff245739a25","_uuid":"3774dd7c-567b-47e8-8a75-03392b43fb11","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","def generate_data(data,labels):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in data:\n","      # `encode_plus` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      #   (5) Pad or truncate the sentence to `max_length`\n","      #   (6) Create attention masks for [PAD] tokens.\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = MAX_SEQ_LEN,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      \n","      # Add the encoded sentence to the list.    \n","      input_ids.append(encoded_dict['input_ids'])\n","      \n","      # And its attention mask (simply differentiates padding from non-padding).\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Convert the lists into tensors.\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","\n","  return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"033dd0a0-92d5-425e-bcb5-e399f69b4ec9","_uuid":"c6b8d777-0575-42f8-a17b-753b65c2bd02","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n","test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n","\n","print('Original: ', train_sentences[0])\n","print('Token IDs:', train_input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58d4180d-c063-42e8-a260-e80ddadfb8c6","_uuid":"38e7368e-d0cd-484b-8237-ec0bbc1585c5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = BATCH_SIZE # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","test_dataloader = DataLoader(\n","            test_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = BATCH_SIZE # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import logging \n","loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n","for logger in loggers:\n","    if \"transformers\" in logger.name.lower():\n","        logger.setLevel(logging.ERROR)\n","#https://stackoverflow.com/a/78844884"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d126a499-d253-46d6-bb5b-9e6a9c70a2eb","_uuid":"8213c77f-e72e-495e-9285-d26ea2f371fa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5734e9d5-1a54-41ab-9baa-72069c59d7a8","_uuid":"71f3a2ac-c1c1-4b9c-9880-6c1c9e6060d2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae32b164-378a-431c-a07a-88ce6cfb32f7","_uuid":"2cbf9965-0ba6-478f-bb3b-c51bf36a28bf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * EPOCHS\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71f900e2-61e0-498c-9a76-ab6db0656fb7","_uuid":"388e7801-7f04-47fc-82df-563afb49b76c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1978ef6e-9ef8-49ac-945d-2e5219c632b1","_uuid":"aa9a6933-b746-4804-9fa2-8e0828fc547c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9c23aac-59bf-4b3a-912c-3e60a8de0a29","_uuid":"2452da05-7e4a-4cd5-be28-8093fed63e9b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["model.device"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbc4b560-ed9c-4ac8-a25f-1a31a4f60dce","_uuid":"3f167a8a-1f19-4118-ba50-53e618438824","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["scheduler.state_dict()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"88e2db9f-4a30-4c44-abfb-9bce400aeb26","_uuid":"ef08a321-661e-497f-9593-3c64157f066b","trusted":true},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_confusion_matrix(y_pred, y_true):\n","    TP = TN = FP = FN = 0\n","    for (true, pred) in zip(y_true, y_pred):\n","        if true == 1 and pred == 1:\n","            TP += 1\n","        elif true == 0 and pred == 0:\n","            TN += 1\n","        elif true == 1 and pred == 0:\n","            FN += 1\n","        elif true == 0 and pred == 1:\n","            FP += 1\n","    return TN, FP, FN, TP"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e362956-4d87-401c-bae0-0ef4352be57f","_uuid":"f12a2469-3be9-4901-a78b-6f3bed5eabb7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import random\n","import os\n","\n","os.makedirs(\"outputs\", exist_ok=True)\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, EPOCHS):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","    \n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the training loss over all of the batches.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    # Set the model to evaluation mode.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    preds = []\n","    labels = []\n","\n","    # Disable gradient calculation for validation\n","    with torch.no_grad():\n","        # For each batch of validation data...\n","        for batch in test_dataloader:\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Forward pass, calculate logit predictions\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask, \n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the validation loss over all of the batches.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            b_preds = np.argmax(logits, axis=1)\n","            b_labels = b_labels.to('cpu').numpy()\n","\n","            preds.extend(b_preds)\n","            labels.extend(b_labels)\n","\n","            # Calculate the accuracy for this batch\n","            total_eval_accuracy += np.sum(b_preds == b_labels)\n","\n","            nb_eval_steps += 1\n","\n","    df = pd.DataFrame({\"actual_label\": labels, \"predicted_label\": preds})\n","    df.to_csv(f\"outputs/validation_predictions_epoch_{epoch_i}.csv\", index=False)\n","\n","    TN, FP, FN, TP = calculate_confusion_matrix(preds, labels)\n","\n","    # Calculate the average validation loss and accuracy.\n","    avg_val_loss = total_eval_loss / nb_eval_steps\n","    avg_val_accuracy = total_eval_accuracy / len(test_dataset)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Validation Loss': avg_val_loss,\n","            'Validation Accuracy': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'val_manual_TN': TN,\n","            'val_manual_FP': FP,\n","            'val_manual_FN': FN,\n","            'val_manual_TP': TP\n","        }\n","    )\n","    \n","    # if epoch_i % 2:\n","    #     torch.save(model.state_dict(),f\"outputs/BERT-imdb-epoch-{epoch_i}.pt\")\n","\n","print(\"\")\n","print(\"Training complete!\")\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n","\n","# Dump stats to CSV\n","pd.DataFrame(training_stats).to_csv(\"outputs/training_stats.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64dbe041-da64-418e-8ad9-15998a62c979","_uuid":"49b68e7f-b40e-49e2-a0b7-63c92737c4c6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(),f\"outputs/BERT-imdb-final-epoch-{EPOCHS}.pt\")\n","torch.save(optimizer.state_dict(),f\"outputs/BERT-optimizer-imdb-final-epoch-{EPOCHS}.pt\")\n","torch.save(scheduler.state_dict(),f\"outputs/BERT-scheduler-imdb-final-epoch-{EPOCHS}.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["# Upload to Gdrive"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tarfile\n","import os\n","\n","def create_tar_gz(folder_path, output_path):\n","    with tarfile.open(output_path, \"w:gz\") as tar:\n","        tar.add(folder_path, arcname=os.path.basename(folder_path))\n","\n","# Specify the folder path and output path\n","folder_path = \"outputs\"\n","output_path = \"outputs.tar.gz\"\n","\n","# Create the tar.gz file\n","create_tar_gz(folder_path, output_path)\n","\n","GDriveUtils.upload_file_to_gdrive(output_path, DRIVE_FOLDER_ID, creds_stringified=GOOGLE_SERVICE_ACC_TOKEN)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9440534e-4301-4108-97a9-e3fbcf631705","_uuid":"f95fd293-9bdf-43a4-a907-933d236ed196","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Prediction on test set\n","\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in test_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","prediction_set = []\n","\n","for i in range(len(true_labels)):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  prediction_set.append(pred_labels_i)\n","\n","prediction_scores = [item for sublist in prediction_set for item in sublist]\n","\n","print('Preds are ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8adae04d-cfe1-4be7-9a95-5b47bfa8f8d4","_uuid":"a4847853-4059-4b94-97ef-603c60c08d75","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n","\n","cm = confusion_matrix(test_labels, prediction_scores)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","print(\"precision_score:\",precision_score(test_labels, prediction_scores))\n","print(\"recall_score:\",recall_score(test_labels, prediction_scores))\n","print(\"f1_score:\",f1_score(test_labels, prediction_scores, average='macro'))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
